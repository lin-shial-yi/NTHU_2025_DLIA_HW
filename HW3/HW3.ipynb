{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q scikit-learn tqdm\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f' Using device: {device}')"
      ],
      "metadata": {
        "id": "hss100mtW_6P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4732e09-c063-4ce8-a6e2-e5160e9b71af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question1\n"
      ],
      "metadata": {
        "id": "sF4sT3H2OVTA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQA2FLjyBP53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3791ab48-2494-4f1d-bf79-c20ca96c23a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/bottle/ground_truth/': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# !unzip /content/drive/MyDrive/bottle-20250407T063408Z-001.zip\n",
        "!rm -r /content/bottle/ground_truth/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of images used\n",
        "\n",
        "def count_images_in_folder(folder_path):\n",
        "    return len([f for f in os.listdir(folder_path) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
        "\n",
        "base_dir = 'bottle'\n",
        "train_dir = os.path.join(base_dir, 'train', 'good')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "test_classes = ['broken_large', 'broken_small', 'contamination', 'good']\n",
        "\n",
        "total_images = 0\n",
        "train_count = count_images_in_folder(train_dir)\n",
        "total_images += train_count\n",
        "\n",
        "test_counts = {}\n",
        "for cls in test_classes:\n",
        "    cls_path = os.path.join(test_dir, cls)\n",
        "    test_counts[cls] = count_images_in_folder(cls_path)\n",
        "    total_images += test_counts[cls]\n",
        "\n",
        "print(f\"Ë®ìÁ∑¥ÈõÜÂúñÁâáÊï∏Èáè: {train_count}\")\n",
        "print(\"Ê∏¨Ë©¶ÈõÜÂêÑÈ°ûÂà•ÂúñÁâáÊï∏Èáè:\")\n",
        "for k, v in test_counts.items():\n",
        "    print(f\"  - {k}: {v} Âºµ\")\n",
        "print(f\"Á∏ΩÂúñÁâáÊï∏Èáè: {total_images}\")"
      ],
      "metadata": {
        "id": "n9Dv_CUlCeiG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e33d3d7-9d4b-46f1-a87f-65b591f4c49c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ë®ìÁ∑¥ÈõÜÂúñÁâáÊï∏Èáè: 209\n",
            "Ê∏¨Ë©¶ÈõÜÂêÑÈ°ûÂà•ÂúñÁâáÊï∏Èáè:\n",
            "  - broken_large: 20 Âºµ\n",
            "  - broken_small: 22 Âºµ\n",
            "  - contamination: 21 Âºµ\n",
            "  - good: 20 Âºµ\n",
            "Á∏ΩÂúñÁâáÊï∏Èáè: 292\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Image dimensions\n",
        "sample_img_path = os.path.join(train_dir, os.listdir(train_dir)[0])\n",
        "img = Image.open(sample_img_path)\n",
        "print(f\"ÂΩ±ÂÉèÂ∞∫ÂØ∏: {img.size} (width x height)\")"
      ],
      "metadata": {
        "id": "_M6jiBBYKAbL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9756e528-3f94-49fa-af0e-f64151e3c236"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ÂΩ±ÂÉèÂ∞∫ÂØ∏: (900, 900) (width x height)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question2"
      ],
      "metadata": {
        "id": "zoA1_pluOcgs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method 1: Baseline Deep SVDDÔºàSimple CNNÔºâ\n"
      ],
      "metadata": {
        "id": "if8RtwyXh2gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "# 1. Ë∂ÖÂèÉÊï∏\n",
        "BATCH_SIZE = 32\n",
        "IMAGE_SIZE = 128\n",
        "EPOCHS = 20\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# 2. Ë≥áÊñôÂ¢ûÂº∑ + ÂâçËôïÁêÜ\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# 3. ËºâÂÖ•Ë®ìÁ∑¥Ë≥áÊñôÔºàÂè™Êúâ goodÔºâ\n",
        "train_dir = 'bottle/train/good'\n",
        "train_dataset = datasets.ImageFolder(root=os.path.dirname(train_dir), transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# 4. CNN ÁâπÂæµËêÉÂèñÊ®°Âûã\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, out_dim=128):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d(1)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128, out_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "model = SimpleCNN().to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# 5. ÂàùÂßãÂåñ‰∏≠ÂøÉÈªû c\n",
        "def get_init_center(model, loader):\n",
        "    model.eval()\n",
        "    outputs = []\n",
        "    with torch.no_grad():\n",
        "        for x, _ in loader:\n",
        "            x = x.to(DEVICE)\n",
        "            out = model(x)\n",
        "            outputs.append(out)\n",
        "    all_feats = torch.cat(outputs)\n",
        "    return torch.mean(all_feats, dim=0)\n",
        "\n",
        "center_c = get_init_center(model, train_loader).detach()\n",
        "\n",
        "# 6. Ë®ìÁ∑¥ Deep SVDDÔºöÊúÄÂ∞èÂåñËàá‰∏≠ÂøÉ c ÁöÑË∑ùÈõ¢\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for x, _ in train_loader:\n",
        "        x = x.to(DEVICE)\n",
        "        feat = model(x)\n",
        "        loss = torch.mean(torch.sum((feat - center_c) ** 2, dim=1))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {np.mean(losses)}\")\n"
      ],
      "metadata": {
        "id": "KAMZ2xpMh9N_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8645d4b-5770-44b1-aa15-49652e347088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Loss: 0.00035729595141934363\n",
            "Epoch 2/20 - Loss: 0.00013535481931674958\n",
            "Epoch 3/20 - Loss: 7.424688711970313e-05\n",
            "Epoch 4/20 - Loss: 4.010630722664895e-05\n",
            "Epoch 5/20 - Loss: 2.2183275827306454e-05\n",
            "Epoch 6/20 - Loss: 1.5340047996557716e-05\n",
            "Epoch 7/20 - Loss: 1.106273781000969e-05\n",
            "Epoch 8/20 - Loss: 9.03044747246895e-06\n",
            "Epoch 9/20 - Loss: 8.225047232761945e-06\n",
            "Epoch 10/20 - Loss: 7.1645557519722e-06\n",
            "Epoch 11/20 - Loss: 6.6832679164820415e-06\n",
            "Epoch 12/20 - Loss: 6.4873692541108796e-06\n",
            "Epoch 13/20 - Loss: 6.498223813520911e-06\n",
            "Epoch 14/20 - Loss: 7.000891595712996e-06\n",
            "Epoch 15/20 - Loss: 7.488056066254753e-06\n",
            "Epoch 16/20 - Loss: 6.67018931770664e-06\n",
            "Epoch 17/20 - Loss: 5.9951876210107e-06\n",
            "Epoch 18/20 - Loss: 6.028211893343334e-06\n",
            "Epoch 19/20 - Loss: 6.1449779553056165e-06\n",
            "Epoch 20/20 - Loss: 6.2518991269046505e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# validation\n",
        "from PIL import Image\n",
        "from sklearn.metrics import roc_auc_score, classification_report\n",
        "\n",
        "# 1. Âª∫Á´ãÊ∏¨Ë©¶ÈõÜË≥áÊñô\n",
        "test_dir = 'bottle/test'\n",
        "test_classes = ['good', 'broken_large', 'broken_small', 'contamination']\n",
        "\n",
        "test_images = []\n",
        "test_labels = []\n",
        "\n",
        "for cls in test_classes:\n",
        "    folder = os.path.join(test_dir, cls)\n",
        "    for img_name in os.listdir(folder):\n",
        "        if img_name.endswith(('.jpg', '.png', '.jpeg')):\n",
        "            img_path = os.path.join(folder, img_name)\n",
        "            test_images.append(img_path)\n",
        "            test_labels.append(0 if cls == 'good' else 1)  # good = 0, abnormal = 1\n",
        "\n",
        "# 2. ÂÆöÁæ©ÂúñÂÉèÂâçËôïÁêÜ\n",
        "def preprocess_image(path):\n",
        "    image = Image.open(path).convert('RGB')\n",
        "    image = transform(image)\n",
        "    return image\n",
        "\n",
        "# 3. Ë®àÁÆóÊØèÂºµÂúñÁâáÁöÑË∑ùÈõ¢ÂàÜÊï∏\n",
        "model.eval()\n",
        "scores = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for path in tqdm(test_images):\n",
        "        img = preprocess_image(path).unsqueeze(0).to(DEVICE)\n",
        "        feat = model(img)\n",
        "        score = torch.sum((feat - center_c) ** 2, dim=1)  # Ê≠êÂºèË∑ùÈõ¢Âπ≥Êñπ\n",
        "        scores.append(score.item())\n",
        "\n",
        "# 4. Ë®≠ÂÆöÈñæÂÄºÔºà95 percentile of train set distanceÔºâ\n",
        "train_distances = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, _ in train_loader:\n",
        "        x = x.to(DEVICE)\n",
        "        feat = model(x)\n",
        "        dist = torch.sum((feat - center_c) ** 2, dim=1)\n",
        "        train_distances.extend(dist.cpu().numpy())\n",
        "\n",
        "threshold = np.percentile(train_distances, 95)  # Ë™øÊï¥ÈÄôÂÄã‰πüÂèØ‰ª•ÂÅö sensitivity ÂàÜÊûê\n",
        "print(f\"‰ΩøÁî®ÁöÑË∑ùÈõ¢ÈñæÂÄºÔºö{threshold}\")\n",
        "\n",
        "# 5. È†êÊ∏¨ÁµêÊûú & Ë©ï‰º∞\n",
        "preds = [1 if score > threshold else 0 for score in scores]\n",
        "\n",
        "print(\"\\nüìä Classification Report:\")\n",
        "print(classification_report(test_labels, preds, target_names=['good', 'anomaly']))\n",
        "\n",
        "auc = roc_auc_score(test_labels, scores)\n",
        "print(f\"üîç ROC AUC Score: {auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "uyLr0ma6iRcP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "825ba574-8fa2-4fdf-c01c-66fc0f27d6df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 83/83 [00:03<00:00, 25.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‰ΩøÁî®ÁöÑË∑ùÈõ¢ÈñæÂÄºÔºö1.6692018107278273e-05\n",
            "\n",
            "üìä Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        good       0.25      0.75      0.37        20\n",
            "     anomaly       0.77      0.27      0.40        63\n",
            "\n",
            "    accuracy                           0.39        83\n",
            "   macro avg       0.51      0.51      0.39        83\n",
            "weighted avg       0.65      0.39      0.39        83\n",
            "\n",
            "üîç ROC AUC Score: 0.5714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method 2: ResNet18Âèñ‰ª£CNN"
      ],
      "metadata": {
        "id": "PFoOVWBFpYvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "# ==== ÂèÉÊï∏ ====\n",
        "BATCH_SIZE = 32\n",
        "IMAGE_SIZE = 224\n",
        "EPOCHS = 20\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# ==== Ë≥áÊñôÂâçËôïÁêÜ ====\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet ÁöÑÂùáÂÄºËàáÊ®ôÊ∫ñÂ∑Æ\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# ==== ËºâÂÖ• train/good ÂúñÁâá ====\n",
        "train_dir = 'bottle/train/good'\n",
        "train_dataset = datasets.ImageFolder(root=os.path.dirname(train_dir), transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# ==== ‰øÆÊîπ ResNet18 Ê®°Âûã ====\n",
        "class ResNet18FeatureExtractor(nn.Module):\n",
        "    def __init__(self, output_dim=128):\n",
        "        super().__init__()\n",
        "        resnet = models.resnet18(pretrained=True)\n",
        "        modules = list(resnet.children())[:-1]  # ÂéªÊéâÊúÄÂæåÂàÜÈ°ûÂ±§ fc\n",
        "        self.backbone = nn.Sequential(*modules)\n",
        "        self.fc = nn.Linear(512, output_dim)  # Âä†‰∏äËΩâÊèõÂ±§\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = x.view(x.size(0), -1)  # flatten (B, 512)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "model = ResNet18FeatureExtractor().to(DEVICE)\n",
        "\n",
        "# ==== SVDD ÊêçÂ§±ËàáÂÑ™ÂåñÂô® ====\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# ==== ÂàùÂßãÂåñ‰∏≠ÂøÉÈªû c ====\n",
        "def get_init_center(model, loader):\n",
        "    model.eval()\n",
        "    outputs = []\n",
        "    with torch.no_grad():\n",
        "        for x, _ in loader:\n",
        "            x = x.to(DEVICE)\n",
        "            feat = model(x)\n",
        "            outputs.append(feat)\n",
        "    all_feats = torch.cat(outputs)\n",
        "    return torch.mean(all_feats, dim=0)\n",
        "\n",
        "center_c = get_init_center(model, train_loader).detach()\n",
        "\n",
        "# ==== Ë®ìÁ∑¥ Deep SVDD ====\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for x, _ in train_loader:\n",
        "        x = x.to(DEVICE)\n",
        "        feat = model(x)\n",
        "        loss = torch.mean(torch.sum((feat - center_c) ** 2, dim=1))  # L2 Ë∑ùÈõ¢Âπ≥Êñπ\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {np.mean(losses):.4f}\")\n",
        "\n",
        "# ==== Ë®àÁÆóË®ìÁ∑¥Ë≥áÊñôË∑ùÈõ¢‰ª•Ë®≠ÂÆöÈñæÂÄº ====\n",
        "train_distances = []\n",
        "with torch.no_grad():\n",
        "    for x, _ in train_loader:\n",
        "        x = x.to(DEVICE)\n",
        "        feat = model(x)\n",
        "        dist = torch.sum((feat - center_c) ** 2, dim=1)\n",
        "        train_distances.extend(dist.cpu().numpy())\n",
        "\n",
        "threshold = np.percentile(train_distances, 95)\n",
        "print(f\"\\n‚úÖ ‰ΩøÁî®ÁöÑË∑ùÈõ¢ÈñæÂÄºÔºö{threshold}\")\n",
        "\n",
        "# ==== Ê∏¨Ë©¶ ====\n",
        "test_dir = 'bottle/test'\n",
        "test_classes = ['good', 'broken_large', 'broken_small', 'contamination']\n",
        "test_images = []\n",
        "test_labels = []\n",
        "\n",
        "for cls in test_classes:\n",
        "    folder = os.path.join(test_dir, cls)\n",
        "    for img_name in os.listdir(folder):\n",
        "        if img_name.endswith(('.jpg', '.png', '.jpeg')):\n",
        "            img_path = os.path.join(folder, img_name)\n",
        "            test_images.append(img_path)\n",
        "            test_labels.append(0 if cls == 'good' else 1)\n",
        "\n",
        "def preprocess_image(path):\n",
        "    image = Image.open(path).convert('RGB')\n",
        "    image = transform(image)\n",
        "    return image\n",
        "\n",
        "model.eval()\n",
        "scores = []\n",
        "with torch.no_grad():\n",
        "    for path in tqdm(test_images):\n",
        "        img = preprocess_image(path).unsqueeze(0).to(DEVICE)\n",
        "        feat = model(img)\n",
        "        score = torch.sum((feat - center_c) ** 2, dim=1)\n",
        "        scores.append(score.item())\n",
        "\n",
        "# ==== Ë©ï‰º∞ ====\n",
        "preds = [1 if score > threshold else 0 for score in scores]\n",
        "\n",
        "print(\"\\nüìä Classification Report:\")\n",
        "print(classification_report(test_labels, preds, target_names=['good', 'anomaly']))\n",
        "\n",
        "auc = roc_auc_score(test_labels, scores)\n",
        "print(f\"üîç ROC AUC Score: {auc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdYcD3R5sJZm",
        "outputId": "97e7ac1d-88cb-4ed2-827b-28f584cdad25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Loss: 29.6427\n",
            "Epoch 2/20 - Loss: 4.3604\n",
            "Epoch 3/20 - Loss: 0.8684\n",
            "Epoch 4/20 - Loss: 0.4549\n",
            "Epoch 5/20 - Loss: 0.1662\n",
            "Epoch 6/20 - Loss: 0.0784\n",
            "Epoch 7/20 - Loss: 0.0510\n",
            "Epoch 8/20 - Loss: 0.0326\n",
            "Epoch 9/20 - Loss: 0.0248\n",
            "Epoch 10/20 - Loss: 0.0194\n",
            "Epoch 11/20 - Loss: 0.0167\n",
            "Epoch 12/20 - Loss: 0.0148\n",
            "Epoch 13/20 - Loss: 0.0134\n",
            "Epoch 14/20 - Loss: 0.0124\n",
            "Epoch 15/20 - Loss: 0.0116\n",
            "Epoch 16/20 - Loss: 0.0110\n",
            "Epoch 17/20 - Loss: 0.0104\n",
            "Epoch 18/20 - Loss: 0.0098\n",
            "Epoch 19/20 - Loss: 0.0094\n",
            "Epoch 20/20 - Loss: 0.0089\n",
            "\n",
            "‚úÖ ‰ΩøÁî®ÁöÑË∑ùÈõ¢ÈñæÂÄºÔºö0.012546464800834656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 83/83 [00:03<00:00, 24.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        good       1.00      0.25      0.40        20\n",
            "     anomaly       0.81      1.00      0.89        63\n",
            "\n",
            "    accuracy                           0.82        83\n",
            "   macro avg       0.90      0.62      0.65        83\n",
            "weighted avg       0.85      0.82      0.77        83\n",
            "\n",
            "üîç ROC AUC Score: 0.9960\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method 3: Âä†‰∏ädata augmentation"
      ],
      "metadata": {
        "id": "DlcMmR5Iqxaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Áõ∏ÊØîÊñºÊñπÊ≥ï‰∫åÔºåÂè™ÈúÄË¶ÅË™øÊï¥transform„ÄÇ\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n",
        "    transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.9, 1.0), ratio=(0.9, 1.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet normalization\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "QvAh0gx6qeQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== ÂèÉÊï∏ ====\n",
        "BATCH_SIZE = 32\n",
        "IMAGE_SIZE = 224\n",
        "EPOCHS = 20\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# ==== Ë≥áÊñôÂâçËôïÁêÜ ====\n",
        "transform = train_transform\n",
        "\n",
        "# ==== ËºâÂÖ• train/good ÂúñÁâá ====\n",
        "train_dir = 'bottle/train/good'\n",
        "train_dataset = datasets.ImageFolder(root=os.path.dirname(train_dir), transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# ==== ‰øÆÊîπ ResNet18 Ê®°Âûã ====\n",
        "class ResNet18FeatureExtractor(nn.Module):\n",
        "    def __init__(self, output_dim=128):\n",
        "        super().__init__()\n",
        "        resnet = models.resnet18(pretrained=True)\n",
        "        modules = list(resnet.children())[:-1]  # ÂéªÊéâÊúÄÂæåÂàÜÈ°ûÂ±§ fc\n",
        "        self.backbone = nn.Sequential(*modules)\n",
        "        self.fc = nn.Linear(512, output_dim)  # Âä†‰∏äËΩâÊèõÂ±§\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = x.view(x.size(0), -1)  # flatten (B, 512)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "model = ResNet18FeatureExtractor().to(DEVICE)\n",
        "\n",
        "# ==== SVDD ÊêçÂ§±ËàáÂÑ™ÂåñÂô® ====\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# ==== ÂàùÂßãÂåñ‰∏≠ÂøÉÈªû c ====\n",
        "def get_init_center(model, loader):\n",
        "    model.eval()\n",
        "    outputs = []\n",
        "    with torch.no_grad():\n",
        "        for x, _ in loader:\n",
        "            x = x.to(DEVICE)\n",
        "            feat = model(x)\n",
        "            outputs.append(feat)\n",
        "    all_feats = torch.cat(outputs)\n",
        "    return torch.mean(all_feats, dim=0)\n",
        "\n",
        "center_c = get_init_center(model, train_loader).detach()\n",
        "\n",
        "# ==== Ë®ìÁ∑¥ Deep SVDD ====\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for x, _ in train_loader:\n",
        "        x = x.to(DEVICE)\n",
        "        feat = model(x)\n",
        "        loss = torch.mean(torch.sum((feat - center_c) ** 2, dim=1))  # L2 Ë∑ùÈõ¢Âπ≥Êñπ\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {np.mean(losses):.4f}\")\n",
        "\n",
        "# ==== Ë®àÁÆóË®ìÁ∑¥Ë≥áÊñôË∑ùÈõ¢‰ª•Ë®≠ÂÆöÈñæÂÄº ====\n",
        "train_distances = []\n",
        "with torch.no_grad():\n",
        "    for x, _ in train_loader:\n",
        "        x = x.to(DEVICE)\n",
        "        feat = model(x)\n",
        "        dist = torch.sum((feat - center_c) ** 2, dim=1)\n",
        "        train_distances.extend(dist.cpu().numpy())\n",
        "\n",
        "threshold = np.percentile(train_distances, 95)\n",
        "print(f\"\\n‚úÖ ‰ΩøÁî®ÁöÑË∑ùÈõ¢ÈñæÂÄºÔºö{threshold}\")\n",
        "\n",
        "# ==== Ê∏¨Ë©¶ ====\n",
        "test_dir = 'bottle/test'\n",
        "test_classes = ['good', 'broken_large', 'broken_small', 'contamination']\n",
        "test_images = []\n",
        "test_labels = []\n",
        "transform = test_transform\n",
        "for cls in test_classes:\n",
        "    folder = os.path.join(test_dir, cls)\n",
        "    for img_name in os.listdir(folder):\n",
        "        if img_name.endswith(('.jpg', '.png', '.jpeg')):\n",
        "            img_path = os.path.join(folder, img_name)\n",
        "            test_images.append(img_path)\n",
        "            test_labels.append(0 if cls == 'good' else 1)\n",
        "\n",
        "def preprocess_image(path):\n",
        "    image = Image.open(path).convert('RGB')\n",
        "    image = transform(image)\n",
        "    return image\n",
        "\n",
        "model.eval()\n",
        "scores = []\n",
        "with torch.no_grad():\n",
        "    for path in tqdm(test_images):\n",
        "        img = preprocess_image(path).unsqueeze(0).to(DEVICE)\n",
        "        feat = model(img)\n",
        "        score = torch.sum((feat - center_c) ** 2, dim=1)\n",
        "        scores.append(score.item())\n",
        "\n",
        "# ==== Ë©ï‰º∞ ====\n",
        "preds = [1 if score > threshold else 0 for score in scores]\n",
        "\n",
        "print(\"\\nüìä Classification Report:\")\n",
        "print(classification_report(test_labels, preds, target_names=['good', 'anomaly']))\n",
        "\n",
        "auc = roc_auc_score(test_labels, scores)\n",
        "print(f\"üîç ROC AUC Score: {auc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQ-Pq-a5tUOO",
        "outputId": "d54bb75c-91d1-4069-e931-69a3986b292b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Loss: 30.4516\n",
            "Epoch 2/20 - Loss: 7.4997\n",
            "Epoch 3/20 - Loss: 2.2265\n",
            "Epoch 4/20 - Loss: 0.9296\n",
            "Epoch 5/20 - Loss: 0.5097\n",
            "Epoch 6/20 - Loss: 0.2991\n",
            "Epoch 7/20 - Loss: 0.1697\n",
            "Epoch 8/20 - Loss: 0.1187\n",
            "Epoch 9/20 - Loss: 0.0904\n",
            "Epoch 10/20 - Loss: 0.0767\n",
            "Epoch 11/20 - Loss: 0.0620\n",
            "Epoch 12/20 - Loss: 0.0580\n",
            "Epoch 13/20 - Loss: 0.0520\n",
            "Epoch 14/20 - Loss: 0.0475\n",
            "Epoch 15/20 - Loss: 0.0447\n",
            "Epoch 16/20 - Loss: 0.0422\n",
            "Epoch 17/20 - Loss: 0.0393\n",
            "Epoch 18/20 - Loss: 0.0359\n",
            "Epoch 19/20 - Loss: 0.0349\n",
            "Epoch 20/20 - Loss: 0.0335\n",
            "\n",
            "‚úÖ ‰ΩøÁî®ÁöÑË∑ùÈõ¢ÈñæÂÄºÔºö0.048705387860536575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 83/83 [00:03<00:00, 22.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        good       0.83      1.00      0.91        20\n",
            "     anomaly       1.00      0.94      0.97        63\n",
            "\n",
            "    accuracy                           0.95        83\n",
            "   macro avg       0.92      0.97      0.94        83\n",
            "weighted avg       0.96      0.95      0.95        83\n",
            "\n",
            "üîç ROC AUC Score: 0.9889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method 4: Êää Ê≠êÂºèË∑ùÈõ¢Âπ≥Êñπ ÊîπÊàêÁî® Mahalanobis Ë∑ùÈõ¢"
      ],
      "metadata": {
        "id": "F3r0vXI5uwpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import inv\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from PIL import Image\n",
        "\n",
        "# üîπ 1. Âª∫Á´ãÊ∏¨Ë©¶Ë≥áÊñôÂàóË°®ËàáÊ®ôÁ±§\n",
        "test_dir = 'bottle/test'\n",
        "test_classes = ['good', 'broken_large', 'broken_small', 'contamination']\n",
        "test_images = []\n",
        "test_labels = []\n",
        "\n",
        "for cls in test_classes:\n",
        "    folder = os.path.join(test_dir, cls)\n",
        "    for img_name in os.listdir(folder):\n",
        "        if img_name.endswith(('.jpg', '.png', '.jpeg')):\n",
        "            img_path = os.path.join(folder, img_name)\n",
        "            test_images.append(img_path)\n",
        "            test_labels.append(0 if cls == 'good' else 1)\n",
        "\n",
        "# üîπ 2. ‰ΩøÁî®ËàáË®ìÁ∑¥Áõ∏ÂêåÁöÑ transform\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "def preprocess_image(path):\n",
        "    image = Image.open(path).convert('RGB')\n",
        "    image = test_transform(image)\n",
        "    return image\n",
        "\n",
        "# üîπ 3. ÂèñÂæóË®ìÁ∑¥ÁâπÂæµÂêëÈáè ‚Üí Ë®àÁÆó mean & covariance\n",
        "model.eval()\n",
        "train_features = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, _ in train_loader:\n",
        "        x = x.to(DEVICE)\n",
        "        feat = model(x)\n",
        "        train_features.append(feat.cpu())\n",
        "\n",
        "train_features = torch.cat(train_features, dim=0).numpy()\n",
        "mean_vec = np.mean(train_features, axis=0)\n",
        "cov_mat = np.cov(train_features, rowvar=False)\n",
        "\n",
        "# Ê≠£ÂâáÂåñÈÅøÂÖçÂ•áÁï∞Áü©Èô£\n",
        "eps = 1e-6\n",
        "cov_mat += eps * np.eye(cov_mat.shape[0])\n",
        "inv_cov = inv(cov_mat)\n",
        "\n",
        "# üîπ 4. ÂÆöÁæ© Mahalanobis Ë∑ùÈõ¢Ë®àÁÆó\n",
        "def mahalanobis_distance(x, mean_vec, inv_cov):\n",
        "    diff = x - mean_vec\n",
        "    return np.sqrt(np.dot(np.dot(diff, inv_cov), diff.T))\n",
        "\n",
        "# üîπ 5. Ê∏¨Ë©¶ÊâÄÊúâ test ÂúñÂÉèÔºåË®àÁÆó Mahalanobis Ë∑ùÈõ¢\n",
        "mahalanobis_scores = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for path in tqdm(test_images):\n",
        "        img = preprocess_image(path).unsqueeze(0).to(DEVICE)\n",
        "        feat = model(img).cpu().numpy().squeeze()\n",
        "        score = mahalanobis_distance(feat, mean_vec, inv_cov)\n",
        "        mahalanobis_scores.append(score)\n",
        "\n",
        "# üîπ 6. Ë®≠ÂÆöÈñæÂÄºÔºà95% Ë®ìÁ∑¥Ë≥áÊñô Mahalanobis Ë∑ùÈõ¢Ôºâ\n",
        "train_distances = [mahalanobis_distance(f, mean_vec, inv_cov) for f in train_features]\n",
        "threshold = np.percentile(train_distances, 95)\n",
        "print(f\"\\n‚úÖ Mahalanobis ÈñæÂÄºÔºö{threshold:.4f}\")\n",
        "\n",
        "# üîπ 7. È†êÊ∏¨ & Ë©ï‰º∞\n",
        "preds = [1 if s > threshold else 0 for s in mahalanobis_scores]\n",
        "\n",
        "print(\"\\nüìä Classification Report:\")\n",
        "print(classification_report(test_labels, preds, target_names=['good', 'anomaly']))\n",
        "\n",
        "auc = roc_auc_score(test_labels, mahalanobis_scores)\n",
        "print(f\"üîç ROC AUC Score: {auc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZtTsbGAtkuj",
        "outputId": "35d96c3b-fba7-4dfe-d754-745704c92abf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 83/83 [00:03<00:00, 24.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Mahalanobis ÈñæÂÄºÔºö12.1587\n",
            "\n",
            "üìä Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        good       0.00      0.00      0.00        20\n",
            "     anomaly       0.76      1.00      0.86        63\n",
            "\n",
            "    accuracy                           0.76        83\n",
            "   macro avg       0.38      0.50      0.43        83\n",
            "weighted avg       0.58      0.76      0.66        83\n",
            "\n",
            "üîç ROC AUC Score: 0.9881\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}